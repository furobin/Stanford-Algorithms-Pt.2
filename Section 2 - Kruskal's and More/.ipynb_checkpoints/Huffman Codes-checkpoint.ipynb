{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Skipped Advanced Union-Find (optional series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction and Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary Code - maps each character of an alphabet Sigma to a binary string. \n",
    " - EX: Sigma = a to z and various punctuation (size 32 overall for example).To encode, use the 32 5-bit binary strings to encode this Sigma (2^5 = 32). \n",
    "     - Can we do better? Yes. If some characters of Sigma are much more frequent than others, using a variable-length code. \n",
    "     \n",
    "Ambiguity EX: Suppose Sigma = {A,B,C,D}. Fixed length encding would be {00, 01, 10, 11}. 2 bit encoding.\n",
    " - Use instead, encoding {0, 01, 10, 1} variable length encoding. Try to get away with 1 bit for some charaters. \n",
    " - But, what if someone gives encoding 001. What is original sequence? Not enough informtion to figure out. Can be AAD, AB. Variable length creates ambiguity. Unclear where one symbol starts and where next one begins\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prefix-Free Codes**\n",
    "\n",
    "Problem: With variable-length codes, not clear where one charater ends and next begins\n",
    "\n",
    "Solution: Prefix-Free Codes - Make sure that for every pair, i,j in Sigma, neither of the encodings f(i) f(j) is a prefix of the other\n",
    "\n",
    "Example: Encoding {A,B,C,D}, can encode as {0, 10, 110, 111). So, no code is a \"prefix\" of any other. This can take advantage of non-uniform frequences in an alphabet.\n",
    " - Let's say for frequencesi: A = 0.6, B = 0.25, C = .1, D = 0.05.\n",
    " - Performance = expected bit requirement to encode the symbols\n",
    " - Fixed Length Performance (2 bits each), so 2 bits per character.\n",
    " - Variable Length Performance = 1 * 0.6 + 2 * 0.25 + 3 * (0.1 + 0.05) = 1.55\n",
    " \n",
    "So, we know variable length encoding can improve performance. What variable length code to use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Huffman Codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful to think of codes as trees. \n",
    "\n",
    "Goal: Best binary prefix-free encoding for a given set of character frequencies.\n",
    "\n",
    "Useful Fact: Binary codes are basically binary trees.\n",
    " - EX: Sigma = {A,B,C,D}. Fixed length: Each char of alphabet is leaf in the final tree. Root to Leaf path gives it the encoding. 0 is Left, 1 is Right. A = 00, B = 01, C = 10, D = 11. \n",
    " - With variable encoding (not prefix-free), A = 0, B = 01. So, at first 0, A is at that node. Then going to 1 from there, goes to B. Ambiguity is where you have internal nodes of the tree (i.e. where A is).  \n",
    " - Prefix-Free Code: Not perfectly balanced tree, will only have lables at leaves of the tree. Can draw out A = 0, B = 10, C = 110, D = 111. Each bit goes down one layer of tree. \n",
    " \n",
    "General Idea:\n",
    " - For each i in Sigma, exactly one node is labeled \"i\"\n",
    " - Encoding of each i is bits along path from root to the node \"i\".\n",
    " - Prefix-Free iff labeled nodes = the leaves. (Prefixes means one node is ancester of another).\n",
    " - To decode: Start at root, when see 0, go left, see 1, go right. Whenever find a leaf, returns i for that leaf. Then start at root again. \n",
    "     - Encoding length of i = depth of i in the tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem:\n",
    " - Input: Probability pi for each character i in Sigma\n",
    " - Notation: If T = tree with leaves corresponding to symbols of sigma, then L(T) = Sum(pi * depth of i in T) = avg encoding length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Greedy Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: What's a principled approach for building a tree with leaves corresponding to symbols of Sigma:\n",
    " - Natural but Suboptimal - Top-Down/Divide and Conquer\n",
    "     - Split symbols into Sigma 1, Sigma 2 each with ~ 50% total frequency\n",
    "     - Recursively compute Ti for Sigma i, return the trees onnected at root. \n",
    " - Huffman's Optimal Idea:\n",
    "     - Build tree bottom-up, start with leaves of tree and then do successive mergers, each step take two sub-trees and link them together as sub-trees under common internal node.  \n",
    "     - Intuitively clear, systematic way that builds trees w/prescribed set of trees. \n",
    "         - Merge creates a new internal node, then merges 2 subtrees under it. Drops # subtrees by 1. Cool. \n",
    "         - Start with N leaves, do N-1 successive merges. Introduce N-1 new unlabeled internal nodes, creates single tree. \n",
    "     \n",
    "Question: Which pair of symbols is \"safe\" to merge?\n",
    " - Observe: final encoding length of i = # of mergers its subtree endures.\n",
    "     - Mergers increase encoding length of symbols by 1. Gives way to progress with greedy heuristic. \n",
    " - Have N original symbols, must pick 2 to merge. So, should merge symbols least frequent first (increases average by the least). \n",
    " \n",
    "How to Recurse?\n",
    " - 1st iteration of algorithm merges symbols a and b. By merging these two, forces algorithm to output tree where a and b have same parent i.e. siblings). Encodings are identical in length. \n",
    "     - In recursion, treat as same symbol. Introduce new metasybol ab, represent all frequencies of either one. The new frequency of ab will be prob(a) + prob(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HuffMans:\n",
    " - If len(Sigma) = 2, return tree of A,B with one root\n",
    " - Let a,b in Sigma have the smallest frequencies\n",
    " - Let Sigma` = Sigma with a,b, replaced by symbol ab \n",
    " - Define pab = pa + pb\n",
    " - Recursively compute T` (for the alphabet Sigma`) \n",
    " - Extend T` (with leaves same as Sigma`) to a tree T ith leaes Sigma by splitting leaf ab into the two leaves a and b\n",
    " - Return T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correctness Proof"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theorem: Huffman's algorithm computes a binary tree that minimizes the avg encoding length\n",
    " - L(T) = Sum(pi * depth i)\n",
    " \n",
    "Proof (By Induction on size n f alphabet sigma, assume n>= 2):\n",
    " - Base Case: when n = 2, algorithm outputs the optimal tree (1 bit per symbol)\n",
    " - Inductive Step: Fix integer with n = |Sigma| < 2.\n",
    " - By Inductive Hypothesis: Algorithm solves smaller subproblem (for Sigma` < Sigma) optimally\n",
    " \n",
    "Inductive Step:\n",
    " - Let Sigma` = Sigma with a,b,  replaced by meta-symbol ab. a,b smallest freqs, pab = pa + pb. \n",
    "     - With ab, commits a,b to be siblings in final tree. Can slit metanode ab, insert internal node w/children a, b. \n",
    "     - Can go between these two forms, i.e. combine or split ab. \n",
    "     - Let Xab = trees for Sigma that have a,b as siblings. \n",
    " - Important: For every such pair T` and T, L(T) - L(T`) is pa[depth of a in T] + pb[depth of b in T] - pab [depth of ab in T`]. \n",
    "     - Let d = depth of ab in T`\n",
    "     - = (pa + pb)[d + 1] - (pa + pb)(d) = (pa + pd)\n",
    "     - The two trees are p much exactly the same except for a,b being differeint with ab being one level above individual a, b in T. \n",
    "     - Key, the difference between two avg encoding lengths is some constant without depending on which trees we started with. \n",
    "         - Doesnt matter if tree perfectly balanced or lopsided. \n",
    "         - So, avg encoding length preserved up to a universal constant\n",
    "     - Preserves objective function up to a constant\n",
    "         \n",
    "Recall Inductive Hypothesis: Huffman's algorithm computes a tree T`.hat that minimizes L(T`) for Sigma`.\n",
    " - Corresponding Tree T.hat minimizees L(T) for sigma over all trees in Xab. \n",
    " - In minimizing avg encoding length for all feasible solutions for the smallest subprblem, the recursive call is actually minimizing the avg encoding length for the original problem with the original alphabet sigma over subset of the feasible solutions. \n",
    " - Right now, getting best possible scenario amongst some solutions in which A and B happen to be siblings. Bad if there is no optimal solution in which A and B are siblings. \n",
    " \n",
    "Key Lemma: There is an optimal tree for Sigma in Xab (i.e. a and b were safe to merge,  A and B, two lowest freq siblings, are siblings)\n",
    " - Intuition: Can make an optimal tree better by pushing a and b as deep as possible (since a and b have smallest frequencies)\n",
    " - By exchange argument, Let T* be any tree that minimizes LT) for Sigma. \n",
    " - Let x, y be siblings at the deepest level of T*. a,b are somewhere as leaves in tree\n",
    " - Exchange: swapping labels a -> x, b -> y\n",
    " - T.hat is in Xab (where a,b are at bottom) by choice of x,y\n",
    " - Will show that L(T.hat) <= L(T*). In doing so, shoes T.hat also optimal, completes proof.\n",
    "     - Reason: the depths of a,b and x,y swapped. \n",
    "     - L(T*) - L(T.hat) = (px - pa) * (depth of x in T* - depth of a in T*) + (py - pb) * (depth of y in T* - depth of b in T*)\n",
    "     - Since a and b have lowest possible frequencies, px - pa; py - pb both >= 0. \n",
    "         - depth of x in T* - depth of a in T* and with y and b also >= 0.\n",
    "         - Since all non-negative, the sum of them all are also nonnegative.\n",
    "         - Thus, L(T*) >= L(T.hat) which means T.hat is also optimal\n",
    "         \n",
    "QED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation and Running Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Implementation: O(n^2) where n = |Sigma|\n",
    " - Total recursive calls is linear to size of alphabet\n",
    " - Each call, searches for minimum frequency symbols A and B, so also linear time. Thus, n^2\n",
    " - Note the minimum computation calculations, look to Heap\n",
    " \n",
    "Speed-Up with Heap (to perform repeated min comp):\n",
    " - Use keys = frequenies\n",
    " - Afte extracting the tw smallest-frequency symbols, re-Insert the new meta-symbol. \n",
    " - Iterative, O(nlogn) implementation\n",
    " - Even Faster: (non-trivial) Sorting + O(n) additional work. Stil O(nlogn) but with smaller constants. \n",
    "     - If sorting as pre-processing step, do not need to use Heap data structure for this. Can use a queue, likely use two queues. \n",
    "     - 1 sort, linear work with 2 queues after. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
