{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Maximum Cut Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input - An undirected graph G \n",
    "\n",
    "Goal: Produce a cut A,B - a partition of V into two non-empty sets - that maximizes the number of crossing edges\n",
    " - NP complete RIP\n",
    " - Tractable speial case: bipartite graph i.e. exists a cut where every edge is crossing\n",
    "     - Use BFS at arbitrary vertex. Draw BFS tree. Put even layers as group A, odd layers as group B. Will have no crossing edges iff graph is bipartite. \n",
    "     \n",
    "A Local Search Algorithm:\n",
    " - Notation: For a cut A,B and a vertex v, define:\n",
    "     - Cv(A,B) = # of edges incident on v that gross A,B\n",
    "     - Dv(A,B) = # of edges incident on v that don't cross (A,B) \n",
    " - 1. Let A,B be an arbitrary cut of G\n",
    " - 2. While there is a vertex v with Dv(A,B) > Cv(A,B):\n",
    "     - move v to the other cut (basically swaps values of Cv(A,B) and Dv(A,B), increases # crossing edges by Dv - Cv)\n",
    " - 3. Return final cut (A,B) \n",
    " - Usually, local search perform poorly with correctness and running time.However, not with this\n",
    " - If at most nC2 edges crossing cut, since each iteration of local search increases # of crossing edges, then terminates within nC2 iterations (hence in polynomial time bc each iteration solved quickly)\n",
    "     - Even though exponential # of different cuts, all different cuts only take on polynomial # of different objective function values (i.e. between 0 and nC2). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theorem: Local search algo always outputs a cut in which the # of  crossing edges is at least 50% of the maximum possible (even >= 50% of |E|)\n",
    " - This algorithm is tight, 50% cannot be inrpoved any more without tweaks to algorithm itself. Be cautious of this only 50% of maximum number possible. \n",
    "     - Expected number of crossing edges of a random cut is already 1/2 |E|\n",
    "     - Proof: Consider a random cut (A,B). For edge e in E, define Xe = 1 if e crosses, 0 if e does not cross \n",
    "         - Consider edge e with ends (u,v). There is 1/2 chance that the ends are on opposite sides. So, E[Xe] = 1/2. \n",
    "         - Via linearity of expectation, sum(E[Xe]) = |E|/2. Sad. \n",
    "     - Still, very difficult to get past 50% guarantee in polynomial time. \n",
    "\n",
    "Proof of Performance Guarantee:\n",
    " - Let A,B be a locally optimal cut (via above)\n",
    " - Then, for every vertex v, Dv <= Cv. \n",
    " - Summing over all v in V: sum(Dv) <= sum(Cv). \n",
    "     - sum(Cv) counts each crossing edge exactly twice (once when the vertex is v, once when vertex is w for (v,w) edge)\n",
    "     - same idea, sum(Dv) counts each non-crossing edge twice\n",
    "     - So: 2 * # of non-crossing <= 2 * # of crossing edges\n",
    "     - 2 - |E| <= 4 * # of crossing edges (add 2 * # crossing edges both sides)\n",
    "     - |E|/2 <= # of crossing edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Weighted Maximum Cut Problem:\n",
    " - Generalization - each edge e has a nonnegative weight. Want to maximize total weight of crossing edges\n",
    " - Local Search still well defined:\n",
    "     - Dv refers to weight of non-crossing edges, Cv refers to weight of crossing\n",
    " - Performance guarantee of 50% still holds for locally optimal cuts (still same for random cuts)\n",
    " - Can have exponential # of cuts with eponential number of distinct objective function values\n",
    "     - Thus, no longer converge in polynomial time unlike non-weighted maximum cut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principles of Local Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neighborhoods:\n",
    " - Let X = set of candidate solutions to a problem\n",
    "     - EX: Maximum Cute problem, X = cuts of G; TSP tours; CSP variable assignments, etc.\n",
    " - Neighborhood Definition: for each x in X, specify which y in X are its \"neighbors\"\n",
    "     - In Max-Cut, x,y are neighboring cuts that differ by moving one vertex. \n",
    "     - For variable assignment, x,y are neighboring variable assignments iff differ in the value of a single variable\n",
    "     - For TSP, x,y are neighboring tours iff differ by 2 edges (i.e. minimum possible difference in # of edges) \n",
    "     \n",
    "Generic Local Search Algorithm:\n",
    " - Let x = some initial solution\n",
    " - While the current solution x has a superior neighboring solution y:\n",
    "     - set x = y (i.e. switching vertex from cut to cut in Max Cut)\n",
    " - return the final (locally optimal) solution x\n",
    " \n",
    "Some Questions:\n",
    " - How to pick initial solution x?\n",
    "     - Situations when using local search:\n",
    "         - 1. Relying almost entirely on local search to solve the problem.\n",
    "         - 2. Have some heuristic already, using local search as a post-processing check of sorts. \n",
    "     - 1. Use a random solution\n",
    "         - Run many independent trials of local search, return the best locally optimal solution found. Ties to situation 1.\n",
    "     - 2. Use best heuristic (via some greedy heuristic or other). \n",
    "         - Use local search as a post processing step to make your solution even better. Ties to situation 2. \n",
    " - If there are several superior neighboring y, which one do we choose?\n",
    "     - Very domain dependent and may have to do testing on your own subject to whatever problem you have\n",
    "     - In general, when using local search to generate approx solution to a problem, want to use randomization (i.e. case 1 above). \n",
    "         - 1. More randomness: choose y at random if multiple surperior neighbors\n",
    "         - 2. Go to best y (one that has the largest benefit over x). \n",
    " - How do we define the neighborhodds?\n",
    "     - Also very domain dependent, again need to do own empirical analysis and exploration\n",
    "     - How big do we make the neighborhood though? \n",
    "         - EX with max cut, what if considering switching 2 vertices instead?\n",
    "             - Well, will have a quadratic number of possibilities to look through instead of linear num possibilities\n",
    "         - Bigger neighborhood = more time required to search for an improving solution\n",
    "         - Larger neighborhood will have fewer local optima, and some will be bad solutions that are good to get rid of. \n",
    "             - May be better in moving towards the global maxima. Better performance/accuracy potentials. \n",
    "     - Basically a question of solution quality vs. computational resource usage\n",
    "         - Bigger neighborhoods support better quality but more resource usage (both time and hardware)\n",
    "         - Find sweet spot for your specific problem. \n",
    " - Is local search guaranteed to terminate eventualy?\n",
    "     - If X (set of possible solutions) is finite and every local step improves some objective function, then yes. Eventually will run out of finite possible things to try. But not impressive on its own, brute-force search does too.\n",
    " - Is local search guaranteed to terminate quickly (i.e. polynomial time)\n",
    "     - Usually not (in theory). Max-Cut non-weighted was a fairly special case\n",
    "     - Usually though it does in practice quick enough. Look up \"Smoothed Analysis\"\n",
    "     - Can also just stop search early, return best solution found so far. \n",
    " - Are locally optimal solutions generally good approximations to globally optimal ones?\n",
    "     - Also no. There will usually exist locally optimal solutions pretty far from globally optimal solutions.\n",
    "     - That is why, when using as primary optimization algorithm, will run many times with random decisions to try and get globally optimal. Best explore space of all solutions and find the best one p much. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-Sat Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input - n Boolean variables, x1 to xn\n",
    "- m clauses of 2 literals each (\"literal\" = x1 or not x1 aka ^x1)\n",
    "    - EX: (x1 v x2); v is logical or. Satisfies if either x1 or x2 is true. ; ^ denotes \"and\" (upside down v, diff from no6 ^)\n",
    "\n",
    "Output:\n",
    " - \"Yes\" if tehre is an assignment that simultaneously satisfies every clause, \"no\" otherwise\n",
    "\n",
    "(In)tractibility:\n",
    " - Can be solved in polynomial time (unlike other constraint satisfaction like 3-Sat)\n",
    " - Reduction to computing strongly connected components\n",
    " - \"Backtracking\" works in polynomial tme\n",
    "     - Start with variable x1; tentatively assign to true. Through clauses, x1=true will have ramifications for rest of variables. Follow through clauses. If reach a contradiction, x1 cannot be true. Backtrack and retry with x1 = false. \n",
    "         - If x1 = true consistently propogates all contraints, then can try to extend satisfying assignment, walking up to a diff variable and setting that tentatively to be true. ????\n",
    " - Randomized Local Search\n",
    " - 3-SAT: Cannonical NP-complete problem, potential solutions\n",
    "     - Brute Force Search - 2^n time\n",
    "     - Rand Local Search - (4/3)^n time. Not covered in depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Papadimitriou's 2-Sat Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat log2n times:\n",
    " - Do local search with randomization per iteration\n",
    "     - X = set of solutions i.e. possible assignments of n variables. \n",
    "     - neighboring solution = differ in flip of a value of a single variable \n",
    " - Choose random initial assignment (uniform)\n",
    " - repeat 2n^2 times:\n",
    "     - if current assignment satisfies all claases, halt and report this\n",
    "     - Consider current assignment not satisfy claases:\n",
    "         - pick arbitrary unsatisfied clause, flip the value of one of its variables (50-50 choice here; can think of other heuristics for this flip though) \n",
    "             - Consider, this flip can mess up a lot of different constraints. \n",
    " - Report \"unsatisfiable\" if not found in all log2n interations (note, this is arrogant bc may not be true)\n",
    "\n",
    "Good Points:\n",
    " - Runs in polynomial time\n",
    " - Always correct if feed into it an unsatisfiable instance. \n",
    "\n",
    "But, what if there's a satisfying assignment? Will the Algo find one? That is, with probability close to 1, will this come up with a satisfying assignment? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Walks on a Line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a key concept to analyzing Papadimitriou's algorithm:\n",
    "\n",
    "Random Walks on Nonnegative integers - Setup:\n",
    " - Line from 0, 1, ... n, n+1, ..... At time 0, begin at position 0.\n",
    " - At each time step, your position goes up or down by 1 with 50/50 probability (except at 0, always goes up). Let's say position n is the goal. \n",
    "     - Let's say goal is 3. Can go from 0 to 1, then 1 to 2, then 2 to 1, then 1 to 2, then 2 to 3. Total 5 time steps to get from 0 to 3. \n",
    "     - For an integer n >= 0, E[Tn] is about Theta(n^2) where Tn is amount of time needed to get to n. \n",
    "     \n",
    "Analysis of Tn:\n",
    " - Consider some position i where Zi = number of random walk time-steps to get from n to i. Note, Z0 = Tn.\n",
    " - Edge Cases:\n",
    "     - E(Zn) = 0 (0 steps to get from n to n)\n",
    "     - E(Z0) = 1 + E[Z1]; first step from 0 to 1 then E[Z1]\n",
    "     - For i in 1,2,3...n-1:\n",
    "         - E[Zi] = P(go left) * E[Zi|go left] + P(go right) * E[Zi|go right]\n",
    "         - = 0.5 * (1 + E[Zi - 1]) + 0.5 * (1 + E[Zi + 1])\n",
    "         - = 1 + 0.5(E[Zi + 1] + E[Zi - 1])\n",
    "         - So, E[Zi] - E[Zi + 1] = E[Zi - 1] - E[Zi] + 2\n",
    "             - Note, E[Zi] decreases as i increases. So, both sides of equation are positive. \n",
    "             - Consider consecutive pair of starting points. As slide pair of points closer to destination, the advantage gets amplified. However much savings get from starting at i instead of i - 1, get that same advantage + 2 if starting at i + 1 instead of i. \n",
    " - So: E[Z0] - E[Z1] = 1. Via above, E[Z2] - E[Z1] = 3. Apply over and over again...\n",
    " - Indices are inreased by n - 1 so E[Zn - 1] - E[Zn] = 2n - 1.\n",
    " - Cancellations occur so E[Z0] - E[Zn] = 1 + 3 + 5 + ... + 2n - 1; E[Zn] = 0\n",
    "     - Think of sums as pairs, so (1 + 2n - 1+ + (3 + 2n - 3) .... so forth.\n",
    "     - If n is even, n/2 number of pairs bc 2 rows. If n is odd, get n/2 pairs + median value which will have value of n.\n",
    "     - So, get E[Z0] = n^2. \n",
    "     \n",
    "Corollary:\n",
    " - Pr(Tn > 2n^2) <= 1/2 (special case of Markov's inequality)\n",
    " - Proof: Let p denote Pr(Tn > 2n^2)\n",
    "     - We have n^2 = E[Tn] by last claim\n",
    "     - E[Tn] = n^2 = sum(k * Pr(Tn = k)) for k in 0 to 2n^2 + sum(k * Pr(Tn = k)) for k in 2n^2 + 1 to infinity (i.e. all values that Tn may ever take on).\n",
    "         - First sum is non-negative because k is not negative, Prob is not negative\n",
    "         - 2nd sum, k >= 2n^2 p much\n",
    "         - So, E[Tn] >= 2n^2 (lower bounded by 0) * Pr(Tn > 2n^2) = 2n^2p\n",
    "         - Thus, p <= 1/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of Papadimitriou's Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theorem: For a satisfiable 2-SAT instance with n variables, Papadimitriou's algorithm produces a satisfying assignment with probability >= 1 - 1/n.\n",
    "\n",
    "Proof: First focus on a single iteration of the outer for loop. Each one is a fresh opportunity to discover a satisfying algorithm.\n",
    " - Fix an arbitrary satisfying assignment a*\n",
    " - Let a.t = algorithm's assignment after inner interation t for t in 0 to 2n^2 (a random var). \n",
    " - Let Xt = number of variables on which a.t and a* agree. \n",
    "     - 0 if a.t assigns everything opposite of what in a*, n if a.t = a* and algorithm halts correctly\n",
    " - Key: Suppose a.t is not a satisfying assignment and algorithm picks unsatisfied clause with variables xi, xj. \n",
    "     - Note: Since a* is satisfying, it makes a different assignment to xi or xj than a.t (or both). \n",
    " - Consequence of algorithm's random variable flip:\n",
    "     - 1. If a* and a.t differ on both xi and xj, then Xt+1 = (Xt) + 1\n",
    "         - That is, in next iteration, a.t+1 will agree on one more variable with a*\n",
    "     - 2. If a* and a.t differ on exactly one of xi or xj:\n",
    "         - If flips the right variable that is different on two, Xt+1 = (Xt) + 1\n",
    "         - If flips the wrong variable i.e. variable with same val: Xt+1 = (Xt) - 1\n",
    "         - 50/50 probability for either case. \n",
    "         - The random variables behave just like random walk on nonnegative integers except that:\n",
    "             - Sometimes it can be more correct with 100% probability (i.e. case 1)\n",
    "             - Might have X0 > 0 instead of X0 = 0 (initial step may have multiple correct variables, almost always true)\n",
    "             - Might stop early before Xt = n.\n",
    "                 - There may be multiple satisfying assignments. Xt measures \"distance\" from a specific a*, but algorithm may fidn a different satisfying assignment which can happen before Xt = n. \n",
    "         - These 3 differences above are all beneficial in that the algoritthm will terminate faster than we might have suspected from random-walk analysis. So, in worst case, algo is equivalent to random walk. \n",
    "         - Thus, in worst case, the probability that algo fails to produce a satisfying assignment in 2n^2 is exactly the same as a random walk failing to reach n in 2n^2 steps <= 1/2. That is, success is > 1/2. \n",
    " - Thus, P(algo fails) <= P(all log2n independent trials fail)\n",
    " - <= (1/2)^log2n\n",
    " - = 1/n; \n",
    " - So, P(success) >= 1 - 1/n\n",
    "     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
